[TOC]

# 2 模型评估与选择

## 2.1 经验误差和过拟合

分类错误的样本数占样本总数的比例称为错误率(error rate). 


$$
若在m个样本中有x个样本分类错误, 则错误率 E = \frac{x}{m}
\\
accuracy\ 精度 = 1 - 错误率
\\
A = 1- \frac{x}{m}
\\
$$

学习器的实际预测输出与样本的真实输出之间的差异称为误差(error).

学习器在训练集上的误差称为训练误差(training error), 或经验误差(empirical error). 在新样本上的误差称为泛化误差(generalization error). 机器学习的最终目标是要得到泛化误差小的学习器.

对训练样本的一般性质没有学习好, 称为欠拟合(underfitting). 欠拟合比较容易客服, 例如在决策树学习中扩展分支, 在神经网络学习中增加训练轮数.

把训练样本自身的一些特点当作所有潜在样本都具有的一般性质, 这样会导致泛化性能下降. 称为"过拟合"(overfitting). 过拟合无法彻底避免, 能做的只有缓解, 或者减小其风险. 机器学习的问题通常是NP, 而有效的学习算法需要在多项式时间内运行完成. 若可以彻底避免过拟合, 则通过经验误差最小化就能获得最优解, 这证明了"P=NP". 因此,  只要相信"P != NP", 过拟合就不可能避免.

## 2.2 模型评估方法

现实任务中, 往往有多种学习算法可供选择, 甚至对于同一个学习算法, 当使用不同的参数配置时, 也会产生不同的模型. 这就是model selection.

为了对不同的模型进行评估, 通常我们可以通过实验测试来对学习器的泛化误差进行评估, 进而做出选择. 

为此, 我们需要一个测试集(testing set) 来测试学习器对新样本的判别能力, 然后以测试集上的测试误差(testing error) 作为泛化误差的近似. 

### 2.2.1 留出法

留出法(hold-out)直接将数据集D划分为两个互斥的集合, 一个作为训练集S, 一个作为测试集T.
$$
D = S\cup  T \\ S\cap T = \varnothing
$$
数据划分时要尽可能保证数据分布的一致性, 如果以采样(sampling) 的角度来看待数据集的划分过程, 则保留类别比例的采样方式通常称为分层采样(stratified sampling). 

在使用留出法时, 一般要采用若干次随机划分, 重复进行实验评估后取平均值作为留出法的评估结果. 例如进行一百次随机划分, 每次产生一个训练/测试集用于试验评估, 100次后就得到100个结果, 而留出法返回的则是这100个结果的平均.

### 2.2.2 交叉验证法

交叉验证法(cross validation) 先将数据集 D 划分为 k 个大小相似的互斥子集,  
$$
D = D_1\cup D_2\cup ... \cup D_k\\
D_i \cap D_j = \varnothing
$$
每个子集都尽可能地保留数据分布的一致性, 即从D中分层采样得到. 

然后, 每次用 k - 1 个子集的并集作为训练集, 余下那个子集作为测试集, 这样可以获得k组训练/测试集.

从而可以进行 k次训练与测试, 最终返回的是这k个测试结果的均值.

交叉验证法评估结果很大程度上取决于k的取值, 也叫k折交叉验证, 通常 k = 10

与留出法相似, 将数据集 D 划分为 k 个子集同样存在多种划分方式, 为减小样本划分方式不同而引入的误差, k折交叉验证通常要随机使用不同的划分重复p次, 最终的评估结果是这p次k折交叉验证结果的均值. 

常见的 10次10折交叉验证法 与 100次留出法 都是进行了 100次训练/测试.

假设数据集有m个样本, 若令 k = m, 则得到交叉验证法的一个特例, 留一法( Leave-One-Out, LOO). 留一法不受随机样本方式的影响, 因为m个样本只有唯一的方式划分为m个子集, 每个子集只包含一个样本. 

但是训练开销很大, 比如数据集有一百万个样本, 则需训练一百万次模型.

### 2.2.3 自助法

我们希望评估的是用D训练出的模型. 但在留出法和交叉验证法中, 由于保留了一部分样本用于测试, 因此实际评估的评估模型所使用的训练集比D小, 这必然会引入一些因训练样本规模不同而导致的估计偏差. 留一法受训练样本规模变化的影响较小, 但计算复杂度太高. 

自助法(bootstrapping)是一个比较好的解决方案. 它直接以自主采样法(bootstrap sampling) 为基础. 给定包含m个样本的数据集D, 对它进行采样产生数据集D', 每次随机从D中挑选出一个样本, 将其拷贝放入D'. 

给定包含m个样本的数据集D, 我们对它进行采样产生的数据集D': 每次随机从D中挑选出一个样本, 将其拷贝放入D', 然后再将该样本放回初始数据集D中, 使得该样本在下次采样时仍然有可能被采到. 这个过程重复执行m次后, 我们就得到了包含m个样本的数据集D'. 这就是自助采样的结果.

 一部分结果会在D'中多次出现, 而另一部分样本不出现. 样本在m次采样中始终不被采到的概率是
$$
(1 - \frac{1}{m})^m \qquad 取极限得到:\\
\lim_{m\mapsto \infty} (1-\frac{1}{m})^m \mapsto \frac{1}{e} \approx 0.368
$$
即通过自助采样, 初始数据集D中约有36.8%的样本未出现在采样数据集D'中. 我们可以将D'用作训练集, D\D'用作测试集. 这样, 实际评估的模型与期望评估的模型都使用m个训练样本, 而我们仍有数据总量约1/3的, 没在训练集中出现的样本用户测试. 这样的测试结果, 亦称为"包外估计" (out-of-bag estimate).

自助法在数据集较小, 难以有效划分训练/测试集时很有用; 此外, 自助法能从初始数据集中产生多个不同的训练集, 这对集成学习等方法有很大的好处. 然而, 自助法产生的数据集改变了初始数据集的分布, 这会引入估计偏差, 因此, 在初始数据集足够时, 留出法和交叉验证法更加常用一些. 

### 2.2.4 调参和最终模型

大多数学习算法, 如果参数不同, 学得模型的性能也有显著差别. 因此, 除了选择合适的学习算法外, 还需要对算法进行设定, 这就是通常所说的"参数调节"(parameter tuning).

比如某个参数设置在[0, 0.2]范围内以0.05为步长, 这样选定的参数未必是最优的, 但这是在计算开销和性能估计之间进行折中妥协的结果. 尽管此处只有5个候选参数值, 但仅算法假定有3个参数值, 每个参数值有5个候选参数可供选择,  整个算法有: 
$$
5^3 = 125 种模型需要考察.
$$
而一个强大的学习算法有海量的参数需要设定, 这需要极大的调参工作量. 

经常忘记的是, 如下的最后一步:

**给定包含m个样本的数据集, 在模型评估与选择过程中由于需要留出一部分数据进行评估测试. 事实上我们只使用了一部分数据训练模型. 因此在模型选择完成后, 学习算法和参数已选定, 此时应该用数据集D重新训练模型, 这个模型在训练过程中使用了所有的m个样本, 这才是我们最终提交给用户的模型.**

我们把学得模型在实际中遇到的数据称为测试数据(testing data), 把模型评估与选择中用于评估测试的数据集称为验证集(validation set). 例如, 在研究对比不同的算法的泛化性能时, 把训练数据另外划分为训练集和验证集, 基于验证集上的性能来进行模型选择和调参. 

## 2.3 性能度量

对学习器的泛化性能进行评估, 不仅需要有效可行的实验估计方法, 还需要有衡量模型泛化能力的评价标准, 这就是性能度量(performance measure).

性能度量反映了 任务需求, 在对比不同模型的能力时, 使用不同的性能度量往往得到不同的评判结果. 

预测任务中, 给定样例集 
$$
D = {(x_1, y_1), (x_2, y_2), ... , (x_m, y_m)}\qquad 其中y_i是示例x_i的真实标记.
$$
要评估学习器f的性能, 就要把学习器预估结果f(x)与真实标记y进行比较.
回归任务中最常用的性能度量是"均方误差"(mean squared error).
$$
E(f; D) = \frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2
$$
对于数据分析D和概率密度函数p(.), 均分误差可描述为
$$
E(f;D) = \int_{x\backsim D}(f(x-y)^2p(x)dx
$$

### 2.3.1 错误率与精度

错误率是分类错误的样本数占样本总数的比例.

精度则是分类正确的样本数占样本总数的比例.
$$
对样例集D,\ 分类错误率\ E(f;D)=\frac{1}{m}\sum_{i=1}^{m}\amalg(f(x_i)\neq y_i)\\
精度\ acc(f;D) = \frac{1}{m}\sum_{i=1}^{m}\amalg(f(x_i) = y_i )\\
= 1 - E(f; D)
$$
更一般的, 对于数据分布D和概率密度函数p(.), 错误率与精度可分布描述为
$$
E(f;D) = \int_{x\sim D}\amalg(f(x) \neq y)p(x) dx\\
acc(f;D) = \int_{x\sim D}\amalg(f(x) = y) p(x) dx\\
 = 1 - E(f;D)  
$$

### 2.3.2 查准率(precision) 查全率(recall)

二分类问题, 可以分为下列情况

- 真正例 TP (true positive)
- 假正例 FP (false positive)
- 真反例 TN (true negative)
- 假反例 FN (false negative)

显然有: TP + FP + TN + FN = 样例总数

分类结果的 混淆矩阵 (confusion matrix) 如下:

| 真实情况 | 预测结果 | 预测结果 |
| :------: | :------: | :------: |
|          |   正例   |   反例   |
|   正例   |    TP    |    FN    |
|   反例   |          |    TN    |

$$
查准率\ P = \frac{TP}{TP+FP}\\
查全率\ R = \frac{TP}{TP+FN}
$$

